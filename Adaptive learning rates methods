
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import copy
import math
import time
from numpy.linalg import inv

def get_AdaM_update(alpha_0, grad, adam_values, b1=.95, b2=.999, e=1e-8):
    adam_values['t'] += 1

    #update
    adam_values['mean'] = b1 * adam_values['mean'] + (1-b1) * grad
    m_hat = adam_values['mean']/(1 - b1**adam_values['t'])

    #update variace
    adam_values['var'] = b2 * adam_values['var'] + (1-b2) * grad**2
    v_hat = adam_values['var']/(1-b2**adam_values['t'])

    return alpha_0 * m_hat/(np.sqrt(v_hat) + e)


# alpha_0 = 1e-3
# adam_values = {'mean': np.zeros(beta.shape), 'var': np.zeros(beta.shape), 't':0}
#
# beta_grad =
# beta_update = get_AdaM_update(alpha_0, beta_grad, adam_values)
# beta += beta_update


def logistic_fn(z):
    return 1/(1 + math.exp(-z))

def load_data_pairs(type_str):
    return pd.read_csv(type_str +"_x.csv").values, pd.read_csv(type_str +"_y.csv")

def run_log_reg_model(x, beta):
    n = x.shape[0]
    y = np.matmul(x,beta)
    #print(np.matmul(x[1,:],beta))
    res = np.zeros((y.shape[0], y.shape[1]))
    for i in range(len(y)):
        res[i] = logistic_fn(y[i])+math.pow(math.e,-100)


    return res

def calc_log_likelihood(x, y, beta):
    n = x.shape[0]
    temp = run_log_reg_model(x, beta)
    matone = np.ones([y.shape[0],1])
    #print(min(temp), max(temp))
    #t = np.log(temp)
    sum = np.matmul(np.transpose(y), np.log(temp)) + np.matmul(np.transpose(matone - y), np.log(matone - temp))

    return sum/n



    # return an avergae, not a sum

def calc_accuracy(x, y, beta):
    theta_hats = run_log_reg_model(x, beta)
    yy = np.zeros((y.shape[0], y.shape[1]))
    for i in range(len(theta_hats)):
        if theta_hats[i] >= 0.5:
            yy[i] = 1
        else:
            yy[i] = 0
    counter = 0
    for j in range(len(y)):
        if y[j] == yy[j]:
            counter = counter + 1
    return counter/y.shape[0]

#########  Model Training

def train_logistic_regression_model(x, y, beta, learning_rate, batch_size, max_epoch):
    beta = copy.deepcopy(beta)
    n_batches = x.shape[0]/batch_size
    n_batches = int(n_batches)
    train_progress = []

    t = 1
    for epoch_idx in range(max_epoch):
        #print("epoch_idx:", epoch_idx)
        for batch_idx in range(n_batches):
            xx = x[(batch_idx * batch_size): (batch_idx * batch_size + (batch_size - 1)), :]
            yy = y[(batch_idx * batch_size): (batch_idx * batch_size + (batch_size - 1))]
            lr = learning_rate/t
            beta_grad = np.matmul(np.transpose(xx), yy - run_log_reg_model(xx, beta))/xx.shape[0]
            beta = beta + lr * beta_grad
            t = t + 1

        train_progress.append(calc_log_likelihood(x, y, beta))
        print("Epoch %d. Train Log Likelihood: %f" %(epoch_idx, train_progress[-1]))
    return beta, train_progress

# def train_logistic_regression_model(x, y, beta, learning_rate, batch_size, max_epoch):
#     beta = copy.deepcopy(beta)
#     n_batches = x.shape[0] / batch_size
#     n_batches = int(n_batches)
#     train_progress = []
#     alpha_0 = 1e-3
#     adam_values = {'mean': np.zeros(beta.shape), 'var': np.zeros(beta.shape), 't': 0}
#
#     for epoch_idx in range(max_epoch):
#
#         for batch_idx in range(n_batches):
#
#             beta_grad = np.matmul(np.transpose(x), y - run_log_reg_model(x, beta)) / x.shape[0]
#             beta_update = get_AdaM_update(alpha_0, beta_grad, adam_values)
#             beta = beta + beta_update
#
#         train_progress.append(calc_log_likelihood(x, y, beta))
#         print("Epoch %d. Train Log Likelihood: %f" % (epoch_idx, train_progress[-1]))
#
#
#
#     return beta, train_progress


# def train_logistic_regression_model(x, y, beta, learning_rate, batch_size, max_epoch):
#     beta = copy.deepcopy(beta)
#     n_batches = x.shape[0]/batch_size
#     n_batches = int(n_batches)
#     train_progress = []
#
#
#
#     for epoch_idx in range(max_epoch):
#
#         for batch_idx in range(n_batches):
#             xx = x[(batch_idx * batch_size): (batch_idx * batch_size + (batch_size - 1)), :]
#             yy = y[(batch_idx * batch_size): (batch_idx * batch_size + (batch_size - 1))]
#             matrixX = xx
#
#             matrixAt = np.zeros((x.shape[0], x.shape[0]))
#             thetahat = run_log_reg_model(xx, beta)
#             thetadiag = np.diag(np.transpose(thetahat)[0])
#             onesdiag = np.identity(xx.shape[0])
#             #start = time.time()
#             matrixAt = np.matmul(thetadiag, onesdiag - thetadiag)
#             # for i in range(x.shape[0]):
#             #     temp = thetahat[i]
#             #     matrixAt[i,i] = temp*(1-temp)
#             #end = time.time()
#             #print('time consumed: ', end - start)
#
#
#             beta_grad = np.matmul(np.transpose(xx), yy - run_log_reg_model(xx, beta))/xx.shape[0]
#
#             temp1 = np.matmul(np.transpose(matrixX), matrixAt)
#             temp2 = np.matmul(temp1, matrixX)
#             temp2 = temp2 + 0.0001 * np.identity(xx.shape[1])
#
#             newton_alpha_t = inv(temp2)############
#
#             beta = beta + np.matmul(newton_alpha_t,beta_grad)
#
#         train_progress.append(calc_log_likelihood(x, y, beta))
#         print("Epoch %d. Train Log Likelihood: %f" %(epoch_idx, train_progress[-1]))
#
#
#     return beta, train_progress

if __name__ == "__main__":

    train_x, train_y = load_data_pairs("train")
    valid_x, valid_y = load_data_pairs("valid")
    test_x, test_y = load_data_pairs("test")

    train_x = np.hstack([train_x, np.ones((train_x.shape[0],1))])
    valid_x = np.hstack([valid_x, np.ones((valid_x.shape[0],1))])
    test_x = np.hstack([test_x, np.ones((test_x.shape[0], 1))])
    train_y = np.hstack([train_y])
    valid_y = np.hstack([valid_y])
    test_y = np.hstack([test_y])

    beta = np.random.normal(loc=0.0, scale =.001, size=(train_x.shape[1],1))

    learning_rates = [0.1]
    batch_sizes = [200]
    max_epochs = 50

    valid_ll = []
    valid_acc = []
    all_params = []
    all_train_logs = []
    all_lrbs = []

    for lr in learning_rates:
        for bs in batch_sizes:

            final_params, train_progress = train_logistic_regression_model(train_x, train_y, beta, lr, bs, max_epochs)
            all_params.append(final_params)
            all_train_logs.append((train_progress, "Learning rate: %f, Batch size: %d" %(lr, bs)))
            all_lrbs.append("Learning rate: %f, Batch size: %d" %(lr, bs))

            valid_ll.append(calc_log_likelihood(valid_x, valid_y, final_params))
            valid_acc.append(calc_accuracy(valid_x, valid_y, final_params))

    best_model_idx = np.argmax(valid_acc)
    best_params = all_params[best_model_idx]
    test_ll = calc_log_likelihood(test_x, test_y, best_params)
    test_acc = calc_accuracy(test_x, test_y, best_params)

    print(all_lrbs)
    print("best_model_idx:" + str(best_model_idx))

    print("Validation Accuracies:" + str(valid_acc))
    print("Test Accuracy: %f" %(test_acc))

    plt.figure()
    epochs = range(max_epochs)
    for idx, log in enumerate(all_train_logs):
        logolist = []
        for e in log[0]:
            logolist.append(float(e[0][0]))

        plt.plot(epochs, logolist, '--', linewidth=3, label="Training," + log[1])
        plt.plot(epochs, max_epochs * [float(valid_ll[idx][0][0])], '-', linewidth=5, label="Validation," + log[1])

    plt.plot(epochs, max_epochs * [float(test_ll[0][0])], '*', ms=8, label="Testing," + all_train_logs[best_model_idx][1])

    plt.xlabel(r"Epoch ($t$)")
    plt.ylabel("Log Likelihood")
    plt.ylim([-0.8, 0.0])
    plt.title("MNIST")
    plt.legend(loc = 4)
    plt.show()
